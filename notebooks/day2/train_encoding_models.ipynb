{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611b305-c71b-4559-8f86-854b9f09acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a26152-a5d6-43f8-ae9c-cefbf77774fc",
   "metadata": {},
   "source": [
    "## Load the model and set up a hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3747336-21c3-400e-ac9b-6b20eb003150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuroai.models import ResNet18\n",
    "\n",
    "backbone_model = ResNet18(pretrained=True, download_root=\"./pretrained_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "092f799b-b225-42a4-b317-9222fe69a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.utils import ForwardHook\n",
    "\n",
    "hook = ForwardHook(model=backbone_model, hook_layer_name = \"model.layer4.0.conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e276be-a521-4316-9232-26f84dd411a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "y = backbone_model(x)\n",
    "\n",
    "## it's of shape (batch, channels, height, width)\n",
    "hook.output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8afce-f9ea-43ad-9f09-670b5aa0b23d",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3a58b357-6b3e-49c3-bfaf-0ae29d9866f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.datasets import NSDAllSubjectSingleRegion\n",
    "\n",
    "device = \"mps:0\"\n",
    "\n",
    "## lets move the model to our device\n",
    "model = backbone_model.to(device)\n",
    "\n",
    "region = \"PPA\"\n",
    "subject_id = \"s1\"\n",
    "\n",
    "dataset = NSDAllSubjectSingleRegion(\n",
    "    folder=\"./datasets/nsd\",\n",
    "    region=region,\n",
    "    transforms=backbone_model.transforms\n",
    ")\n",
    "\n",
    "all_features = []\n",
    "all_fmri_voxels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        image = dataset[i][\"image\"]\n",
    "\n",
    "        ## (voxels) -> (1, voxels)\n",
    "        fmri_data = dataset[i][\"fmri_response\"][subject_id].unsqueeze(0)\n",
    "        \n",
    "        ## (channels, height, width) -> (1, channels, height, width)\n",
    "        image= image.unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        y = backbone_model(image)\n",
    "\n",
    "        ## making sure that we're moving stuff back to the RAM with .cpu()\n",
    "        all_features.append(hook.output.cpu())\n",
    "        all_fmri_voxels.append(fmri_data.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1f23c-940c-4815-bc6e-9d067b15873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_features), len(all_fmri_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c3acc-cf10-43d0-bb53-7f004d7b54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[0].shape, all_fmri_voxels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e45ddb1a-7811-4f6c-a82e-79347265572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = torch.cat(all_features, dim = 0)\n",
    "all_fmri_voxels = torch.cat(all_fmri_voxels, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3486b-3782-4f24-bff0-2d2decf7d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## num samples, channels, height, width\n",
    "print(f\"Shape of features\", all_features.shape)\n",
    "\n",
    "## num samples, voxels\n",
    "print(f\"Shape of voxels\", all_fmri_voxels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6f3ba-8b1c-4578-8862-744e1f39f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "all_features_flattened = rearrange(\n",
    "    all_features,\n",
    "    \"batch channels height width -> batch (channels height width)\"\n",
    ")\n",
    "\n",
    "print(f\"Shape of features after flattening\", all_features_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d7d0efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import reduce\n",
    "all_fmri_voxels_region_mean = reduce(\n",
    "    all_fmri_voxels,\n",
    "    pattern = \"batch voxels -> batch\",\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95b863-7c78-4154-8c9b-064dbde0ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.utils.regression import ridge_regression\n",
    "\n",
    "num_train_samples = 900\n",
    "\n",
    "X = {\n",
    "    \"train\": all_features_flattened[:num_train_samples],\n",
    "    \"test\": all_features_flattened[num_train_samples:],\n",
    "}\n",
    "Y = {\n",
    "    \"train\": all_fmri_voxels_region_mean[:num_train_samples],\n",
    "    \"test\": all_fmri_voxels_region_mean[num_train_samples:],\n",
    "}\n",
    "ridge_result = ridge_regression(\n",
    "    X_train=X[\"train\"],\n",
    "    Y_train=Y[\"train\"],\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(ridge_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "383e3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.utils.regression import RidgeModel\n",
    "\n",
    "model = RidgeModel(\n",
    "    backbone_model=backbone_model,\n",
    "    transforms=backbone_model.transforms,\n",
    "    hook_layer_name=\"model.layer4.0.conv1\",\n",
    "    ridge_result=ridge_result,\n",
    "    device=\"mps:0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = model.evaluate(\n",
    "    x_test=X[\"test\"],\n",
    "    y_test=Y[\"test\"],\n",
    ")\n",
    "print(f\"Correlation on test set: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "47136c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O face.jpg \"https://img.freepik.com/free-photo/portrait-white-man-isolated_53876-40306.jpg\"\n",
    "# !wget -O body.jpg \"https://images.unsplash.com/photo-1586710743237-1eb35c3c881c?fm=jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd2603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
