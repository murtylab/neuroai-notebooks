{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee59ccfb-9e2f-499b-b9ae-8250150dfc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66778cb1",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611b305-c71b-4559-8f86-854b9f09acee",
   "metadata": {
    "id": "2611b305-c71b-4559-8f86-854b9f09acee"
   },
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git --no-deps\n",
    "!pip install git+https://github.com/murtylab/neuroai-notebooks.git --upgrade\n",
    "!mkdir datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a26152-a5d6-43f8-ae9c-cefbf77774fc",
   "metadata": {
    "id": "f0a26152-a5d6-43f8-ae9c-cefbf77774fc"
   },
   "source": [
    "## Load the model and set up a hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3747336-21c3-400e-ac9b-6b20eb003150",
   "metadata": {
    "id": "e3747336-21c3-400e-ac9b-6b20eb003150"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuroai.models import CLIPRN50\n",
    "\n",
    "backbone_model = CLIPRN50(pretrained=True, download_root=\"./pretrained_checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8afce-f9ea-43ad-9f09-670b5aa0b23d",
   "metadata": {
    "id": "a5a8afce-f9ea-43ad-9f09-670b5aa0b23d"
   },
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092f799b-b225-42a4-b317-9222fe69a06b",
   "metadata": {
    "id": "092f799b-b225-42a4-b317-9222fe69a06b"
   },
   "outputs": [],
   "source": [
    "from neuroai.utils import ForwardHook\n",
    "\n",
    "hook = ForwardHook(\n",
    "    model=backbone_model,\n",
    "    hook_layer_name = \"model.layer3.1.conv1\" # << extract activations from this layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e276be-a521-4316-9232-26f84dd411a7",
   "metadata": {
    "id": "26e276be-a521-4316-9232-26f84dd411a7"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "y = backbone_model(x)\n",
    "\n",
    "## it's of shape (batch, channels, height, width)\n",
    "hook.output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626570a",
   "metadata": {},
   "source": [
    "## Downloading the NSD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "zhwnBLbks5-_",
   "metadata": {
    "id": "zhwnBLbks5-_"
   },
   "outputs": [],
   "source": [
    "from neuroai.datasets import download_and_extract_nsd\n",
    "\n",
    "download_and_extract_nsd(zip_filename = \"nsd.zip\", output_folder = \"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58b357-6b3e-49c3-bfaf-0ae29d9866f7",
   "metadata": {
    "id": "3a58b357-6b3e-49c3-bfaf-0ae29d9866f7"
   },
   "outputs": [],
   "source": [
    "from neuroai.datasets import NSDAllSubjectSingleRegion\n",
    "import torch.nn as nn\n",
    "\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"mps:0\"\n",
    "print(f\"Using device: {device}\")\n",
    "## lets move the model to our device\n",
    "backbone_model = backbone_model.to(device)\n",
    "\n",
    "region = \"EBA\"\n",
    "dataset = NSDAllSubjectSingleRegion(\n",
    "    folder=\"./datasets/nsd\",\n",
    "    region=region,\n",
    "    transforms=backbone_model.transforms,\n",
    "    subset=\"train\",\n",
    "    train_test_split=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb6b3d",
   "metadata": {},
   "source": [
    "## Collecting image features and the corresponding voxels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "379c3acc-cf10-43d0-bb53-7f004d7b54bd",
   "metadata": {
    "id": "379c3acc-cf10-43d0-bb53-7f004d7b54bd"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def collect_features_and_voxels(\n",
    "    dataset: NSDAllSubjectSingleRegion,\n",
    "    backbone_model: nn.Module,\n",
    "    hook: ForwardHook,\n",
    "    device: str,\n",
    "    subject_id: str\n",
    "):\n",
    "\n",
    "    ## this is where we dump our data\n",
    "    all_features = []\n",
    "    all_fmri_voxels = []\n",
    "    backbone_model = backbone_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Collecting features and voxels\"):\n",
    "            image: torch.Tensor = dataset[i][\"image\"]\n",
    "\n",
    "            ## (voxels) -> (1, voxels)\n",
    "            fmri_data: torch.Tensor = dataset[i][\"fmri_response\"][subject_id].unsqueeze(0)\n",
    "\n",
    "            ## (channels, height, width) -> (1, channels, height, width)\n",
    "            image = image.unsqueeze(0)\n",
    "            image = image.to(device)\n",
    "            y = backbone_model(image)\n",
    "\n",
    "            ## making sure that we're moving stuff back to the RAM with .cpu()\n",
    "            hook_output = hook.output.cpu()\n",
    "            all_features.append(hook_output)\n",
    "            all_fmri_voxels.append(fmri_data.cpu())\n",
    "\n",
    "    return {\n",
    "        \"features\": torch.cat(all_features, dim=0),\n",
    "        \"voxels\": torch.cat(all_fmri_voxels, dim=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ddb1a-7811-4f6c-a82e-79347265572e",
   "metadata": {
    "id": "e45ddb1a-7811-4f6c-a82e-79347265572e"
   },
   "outputs": [],
   "source": [
    "train_subject_id = \"s2\"\n",
    "\n",
    "data = collect_features_and_voxels(\n",
    "    dataset=dataset,\n",
    "    backbone_model=backbone_model,\n",
    "    hook=hook,\n",
    "    device=device,\n",
    "    subject_id=train_subject_id\n",
    ")\n",
    "## num samples, channels, height, width\n",
    "print(f\"Shape of features\", data[\"features\"].shape)\n",
    "\n",
    "## num samples, voxels\n",
    "print(f\"Shape of voxels\", data[\"voxels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7343ed",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f95b863-7c78-4154-8c9b-064dbde0ba6c",
   "metadata": {
    "id": "1f95b863-7c78-4154-8c9b-064dbde0ba6c"
   },
   "outputs": [],
   "source": [
    "from neuroai.utils.regression import ridge_regression\n",
    "from neuroai.utils.regression import RidgeModel\n",
    "\n",
    "ridge_result = ridge_regression(\n",
    "    x_train=data[\"features\"],\n",
    "    y_train=data[\"voxels\"],\n",
    "    flatten_x=True, ## this squishes features to be (batch, channels*height*width)\n",
    "    predict_mean=False ## set this to True to predict the mean response of a region\n",
    ")\n",
    "\n",
    "model = RidgeModel(\n",
    "    backbone_model=backbone_model,\n",
    "    transforms=backbone_model.transforms,\n",
    "    hook_layer_name=\"model.layer3.1.conv1\", ## make sure that this is the same as the one used in ForwardHook\n",
    "    ridge_result=ridge_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69883585",
   "metadata": {
    "id": "69883585"
   },
   "source": [
    "## Now let's evaluate on data from another subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e475939",
   "metadata": {
    "id": "0e475939"
   },
   "outputs": [],
   "source": [
    "valid_subject_ids = [\"s1\", \"s2\", \"s5\", \"s7\"]\n",
    "\n",
    "eval_subject_id = \"s1\"\n",
    "\n",
    "dataset = NSDAllSubjectSingleRegion(\n",
    "    folder=\"./datasets/nsd\",\n",
    "    region=region,\n",
    "    transforms=backbone_model.transforms,\n",
    "    subset=\"test\", ## << make sure that this is set to \"test\" when evaluating\n",
    "    train_test_split=0.8\n",
    ")\n",
    "\n",
    "data = collect_features_and_voxels(\n",
    "    dataset=dataset,\n",
    "    backbone_model=backbone_model,\n",
    "    hook=hook,\n",
    "    device=device,\n",
    "    subject_id=eval_subject_id\n",
    ")\n",
    "\n",
    "correlation = model.evaluate(\n",
    "    x_test=data[\"features\"],\n",
    "    y_test=data[\"voxels\"],\n",
    "    predict_mean=True\n",
    ")\n",
    "print(f\"Region: {region} Trained on subject: {train_subject_id} Eval on subject: {eval_subject_id}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848a39b",
   "metadata": {
    "id": "7848a39b"
   },
   "source": [
    "## Running the model on your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47136c8d",
   "metadata": {
    "id": "47136c8d"
   },
   "outputs": [],
   "source": [
    "# !wget -O cat.jpg \"https://plus.unsplash.com/premium_photo-1667030474693-6d0632f97029\"\n",
    "!wget -O castle.jpg \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Panorámica_Otoño_Alcázar_de_Segovia.jpg/1200px-Panorámica_Otoño_Alcázar_de_Segovia.jpg\"\n",
    "!wget -O monkey.jpg \"https://images.unsplash.com/photo-1581828060707-37894f1ed9b8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0fd2603",
   "metadata": {
    "id": "d0fd2603"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "images = [\n",
    "        Image.open(\"monkey.jpg\"),\n",
    "        Image.open(\"castle.jpg\"),\n",
    "    ]\n",
    "\n",
    "results = model.run(\n",
    "    images = images\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iqyfZIAyJahE",
   "metadata": {
    "id": "iqyfZIAyJahE"
   },
   "source": [
    "## Bonus: Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4415b4b8",
   "metadata": {
    "collapsed": true,
    "id": "4415b4b8",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install torch-dreams==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sULsgUz7Jcym",
   "metadata": {
    "id": "sULsgUz7Jcym"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torch_dreams import Dreamer\n",
    "\n",
    "dreamy_boi = Dreamer(model, device = device, quiet=False)\n",
    "\n",
    "\n",
    "def maximize_voxel(layer_outputs):\n",
    "    loss = layer_outputs[0].mean()\n",
    "    return -loss\n",
    "\n",
    "image_param = dreamy_boi.render(\n",
    "    layers = [model],\n",
    "    width =224,\n",
    "    height =224,\n",
    "    custom_func = maximize_voxel,\n",
    "    scale_min=1.0,\n",
    "    scale_max=1.0,\n",
    "    iters=300,\n",
    "    translate_x=0.5,\n",
    "    translate_y=0.5,\n",
    "    rotate_degrees=5\n",
    ")\n",
    "\n",
    "plt.imshow(image_param)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "Nn8pVxGrLTXT",
   "metadata": {
    "id": "Nn8pVxGrLTXT"
   },
   "outputs": [],
   "source": [
    "image_param.save(\"visualization_eba.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abf9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
