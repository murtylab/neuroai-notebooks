{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611b305-c71b-4559-8f86-854b9f09acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a26152-a5d6-43f8-ae9c-cefbf77774fc",
   "metadata": {},
   "source": [
    "## Load the model and set up a hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e3747336-21c3-400e-ac9b-6b20eb003150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuroai.models import ResNet18\n",
    "\n",
    "backbone_model = ResNet18(pretrained=True, download_root=\"./pretrained_checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8afce-f9ea-43ad-9f09-670b5aa0b23d",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "092f799b-b225-42a4-b317-9222fe69a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.utils import ForwardHook\n",
    "\n",
    "hook = ForwardHook(\n",
    "    model=backbone_model, \n",
    "    hook_layer_name = \"model.layer4.0.conv1\" # << extract activations from this layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e276be-a521-4316-9232-26f84dd411a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "y = backbone_model(x)\n",
    "\n",
    "## it's of shape (batch, channels, height, width)\n",
    "hook.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3a58b357-6b3e-49c3-bfaf-0ae29d9866f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.datasets import NSDAllSubjectSingleRegion\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"mps:0\"\n",
    "\n",
    "## lets move the model to our device\n",
    "backbone_model = backbone_model.to(device)\n",
    "\n",
    "region = \"PPA\"\n",
    "dataset = NSDAllSubjectSingleRegion(\n",
    "    folder=\"./datasets/nsd\",\n",
    "    region=region,\n",
    "    transforms=backbone_model.transforms,\n",
    "    subset=\"train\",\n",
    "    train_test_split=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "379c3acc-cf10-43d0-bb53-7f004d7b54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features_and_voxels(\n",
    "    dataset: NSDAllSubjectSingleRegion, \n",
    "    backbone_model: nn.Module, \n",
    "    hook: ForwardHook, \n",
    "    device: str, \n",
    "    subject_id: str\n",
    "):\n",
    "    \n",
    "    ## this is where we dump our data\n",
    "    all_features = []\n",
    "    all_fmri_voxels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            image: torch.Tensor = dataset[i][\"image\"]\n",
    "\n",
    "            ## (voxels) -> (1, voxels)\n",
    "            fmri_data: torch.Tensor = dataset[i][\"fmri_response\"][subject_id].unsqueeze(0)\n",
    "\n",
    "            ## (channels, height, width) -> (1, channels, height, width)\n",
    "            image = image.unsqueeze(0)\n",
    "            image = image.to(device)\n",
    "            y = backbone_model(image)\n",
    "\n",
    "            ## making sure that we're moving stuff back to the RAM with .cpu()\n",
    "            hook_output = hook.output.cpu()\n",
    "            all_features.append(hook_output)\n",
    "            all_fmri_voxels.append(fmri_data.cpu())\n",
    "\n",
    "    return {\n",
    "        \"features\": torch.cat(all_features, dim=0),\n",
    "        \"voxels\": torch.cat(all_fmri_voxels, dim=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e45ddb1a-7811-4f6c-a82e-79347265572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"s2\"\n",
    "\n",
    "data = collect_features_and_voxels(\n",
    "    dataset=dataset, \n",
    "    backbone_model=backbone_model, \n",
    "    hook=hook, \n",
    "    device=device, \n",
    "    subject_id=subject_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3486b-3782-4f24-bff0-2d2decf7d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## num samples, channels, height, width\n",
    "print(f\"Shape of features\", data[\"features\"].shape)\n",
    "\n",
    "## num samples, voxels\n",
    "print(f\"Shape of voxels\", data[\"voxels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6f3ba-8b1c-4578-8862-744e1f39f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "all_features_flattened = rearrange(\n",
    "    data[\"features\"],\n",
    "    \"batch channels height width -> batch (channels height width)\"\n",
    ")\n",
    "\n",
    "print(f\"Shape of features after flattening\", all_features_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d7d0efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import reduce\n",
    "all_fmri_voxels_region_mean = reduce(\n",
    "    data[\"voxels\"],\n",
    "    pattern = \"batch voxels -> batch\",\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95b863-7c78-4154-8c9b-064dbde0ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.utils.regression import ridge_regression\n",
    "\n",
    "ridge_result = ridge_regression(\n",
    "    X_train=all_features_flattened,\n",
    "    Y_train=all_fmri_voxels_region_mean,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(ridge_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "383e3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.utils.regression import RidgeModel\n",
    "\n",
    "model = RidgeModel(\n",
    "    backbone_model=backbone_model,\n",
    "    transforms=backbone_model.transforms,\n",
    "    hook_layer_name=\"model.layer4.0.conv1\",\n",
    "    ridge_result=ridge_result,\n",
    "    device=\"mps:0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69883585",
   "metadata": {},
   "source": [
    "## Now let's evaluate on data from another subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e475939",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_subject_ids = [\"s1\", \"s2\", \"s5\", \"s7\"]\n",
    "\n",
    "subject_id = \"s1\"\n",
    "\n",
    "dataset = NSDAllSubjectSingleRegion(\n",
    "    folder=\"./datasets/nsd\",\n",
    "    region=region,\n",
    "    transforms=backbone_model.transforms,\n",
    "    subset=\"test\", ## << make sure that this is set to \"test\" when evaluating\n",
    "    train_test_split=0.8\n",
    ")\n",
    "\n",
    "data = collect_features_and_voxels(\n",
    "    dataset=dataset, \n",
    "    backbone_model=backbone_model, \n",
    "    hook=hook, \n",
    "    device=device, \n",
    "    subject_id=subject_id\n",
    ")\n",
    "all_features_flattened = rearrange(\n",
    "    data[\"features\"],\n",
    "    \"batch channels height width -> batch (channels height width)\"\n",
    ")\n",
    "all_fmri_voxels_region_mean = reduce(\n",
    "    data[\"voxels\"],\n",
    "    pattern = \"batch voxels -> batch\",\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "print(all_features_flattened.shape, all_fmri_voxels_region_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70819a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = model.evaluate(\n",
    "    x_test=all_features_flattened,\n",
    "    y_test=all_fmri_voxels_region_mean,\n",
    ")\n",
    "print(f\"Correlation on another subject {subject_id}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848a39b",
   "metadata": {},
   "source": [
    "## Running the model on your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47136c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O cat.jpg \"https://plus.unsplash.com/premium_photo-1667030474693-6d0632f97029\"\n",
    "# !wget -O castle.jpg \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Panorámica_Otoño_Alcázar_de_Segovia.jpg/1200px-Panorámica_Otoño_Alcázar_de_Segovia.jpg\"\n",
    "# !wget -O monkey.jpg \"https://images.unsplash.com/photo-1581828060707-37894f1ed9b8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "        Image.open(\"monkey.jpg\"),\n",
    "        Image.open(\"castle.jpg\")\n",
    "    ]\n",
    "\n",
    "results = model.run(\n",
    "    images = images\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415b4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
