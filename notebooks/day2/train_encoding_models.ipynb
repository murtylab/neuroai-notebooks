{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee59ccfb-9e2f-499b-b9ae-8250150dfc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611b305-c71b-4559-8f86-854b9f09acee",
   "metadata": {
    "id": "2611b305-c71b-4559-8f86-854b9f09acee"
   },
   "outputs": [],
   "source": [
    "# !pip install ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git --no-deps\n",
    "# !pip install git+https://github.com/murtylab/neuroai-notebooks.git --upgrade\n",
    "# !mkdir datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a26152-a5d6-43f8-ae9c-cefbf77774fc",
   "metadata": {
    "id": "f0a26152-a5d6-43f8-ae9c-cefbf77774fc"
   },
   "source": [
    "## Load the model and set up a hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3747336-21c3-400e-ac9b-6b20eb003150",
   "metadata": {
    "id": "e3747336-21c3-400e-ac9b-6b20eb003150"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuroai.models import ResNet18, CLIPRN50\n",
    "\n",
    "backbone_model = CLIPRN50(pretrained=True, download_root=\"./pretrained_checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8afce-f9ea-43ad-9f09-670b5aa0b23d",
   "metadata": {
    "id": "a5a8afce-f9ea-43ad-9f09-670b5aa0b23d"
   },
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092f799b-b225-42a4-b317-9222fe69a06b",
   "metadata": {
    "id": "092f799b-b225-42a4-b317-9222fe69a06b"
   },
   "outputs": [],
   "source": [
    "from neuroai.utils import ForwardHook\n",
    "\n",
    "hook = ForwardHook(\n",
    "    model=backbone_model,\n",
    "    hook_layer_name = \"model.layer3.1.conv1\" # << extract activations from this layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e276be-a521-4316-9232-26f84dd411a7",
   "metadata": {
    "id": "26e276be-a521-4316-9232-26f84dd411a7"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "y = backbone_model(x)\n",
    "\n",
    "## it's of shape (batch, channels, height, width)\n",
    "hook.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "zhwnBLbks5-_",
   "metadata": {
    "id": "zhwnBLbks5-_"
   },
   "outputs": [],
   "source": [
    "from neuroai.datasets import NSDAllSubjectSingleRegion, download_and_extract_nsd\n",
    "\n",
    "# download_and_extract_nsd(zip_filename = \"nsd.zip\", output_folder = \"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58b357-6b3e-49c3-bfaf-0ae29d9866f7",
   "metadata": {
    "id": "3a58b357-6b3e-49c3-bfaf-0ae29d9866f7"
   },
   "outputs": [],
   "source": [
    "from neuroai.datasets import NSDAllSubjectSingleRegion, download_and_extract_nsd\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "## lets move the model to our device\n",
    "backbone_model = backbone_model.to(device)\n",
    "\n",
    "region = \"EBA\"\n",
    "dataset = NSDAllSubjectSingleRegion(\n",
    "    folder=\"./datasets/nsd\",\n",
    "    region=region,\n",
    "    transforms=backbone_model.transforms,\n",
    "    subset=\"train\",\n",
    "    train_test_split=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379c3acc-cf10-43d0-bb53-7f004d7b54bd",
   "metadata": {
    "id": "379c3acc-cf10-43d0-bb53-7f004d7b54bd"
   },
   "outputs": [],
   "source": [
    "def collect_features_and_voxels(\n",
    "    dataset: NSDAllSubjectSingleRegion,\n",
    "    backbone_model: nn.Module,\n",
    "    hook: ForwardHook,\n",
    "    device: str,\n",
    "    subject_id: str\n",
    "):\n",
    "\n",
    "    ## this is where we dump our data\n",
    "    all_features = []\n",
    "    all_fmri_voxels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            image: torch.Tensor = dataset[i][\"image\"]\n",
    "\n",
    "            ## (voxels) -> (1, voxels)\n",
    "            fmri_data: torch.Tensor = dataset[i][\"fmri_response\"][subject_id].unsqueeze(0)\n",
    "\n",
    "            ## (channels, height, width) -> (1, channels, height, width)\n",
    "            image = image.unsqueeze(0)\n",
    "            image = image.to(device)\n",
    "            y = backbone_model(image)\n",
    "\n",
    "            ## making sure that we're moving stuff back to the RAM with .cpu()\n",
    "            hook_output = hook.output.cpu()\n",
    "            all_features.append(hook_output)\n",
    "            all_fmri_voxels.append(fmri_data.cpu())\n",
    "\n",
    "    return {\n",
    "        \"features\": torch.cat(all_features, dim=0),\n",
    "        \"voxels\": torch.cat(all_fmri_voxels, dim=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e45ddb1a-7811-4f6c-a82e-79347265572e",
   "metadata": {
    "id": "e45ddb1a-7811-4f6c-a82e-79347265572e"
   },
   "outputs": [],
   "source": [
    "subject_id = \"s2\"\n",
    "\n",
    "data = collect_features_and_voxels(\n",
    "    dataset=dataset,\n",
    "    backbone_model=backbone_model,\n",
    "    hook=hook,\n",
    "    device=device,\n",
    "    subject_id=subject_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3486b-3782-4f24-bff0-2d2decf7d97f",
   "metadata": {
    "id": "5be3486b-3782-4f24-bff0-2d2decf7d97f"
   },
   "outputs": [],
   "source": [
    "## num samples, channels, height, width\n",
    "print(f\"Shape of features\", data[\"features\"].shape)\n",
    "\n",
    "## num samples, voxels\n",
    "print(f\"Shape of voxels\", data[\"voxels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6f3ba-8b1c-4578-8862-744e1f39f68d",
   "metadata": {
    "id": "34d6f3ba-8b1c-4578-8862-744e1f39f68d"
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "all_features_flattened = rearrange(\n",
    "    data[\"features\"],\n",
    "    \"batch channels height width -> batch (channels height width)\"\n",
    ")\n",
    "\n",
    "print(f\"Shape of features after flattening\", all_features_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d0efee",
   "metadata": {
    "id": "d7d0efee"
   },
   "outputs": [],
   "source": [
    "from einops import reduce\n",
    "all_fmri_voxels_region_mean = reduce(\n",
    "    data[\"voxels\"],\n",
    "    pattern = \"batch voxels -> batch\",\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95b863-7c78-4154-8c9b-064dbde0ba6c",
   "metadata": {
    "id": "1f95b863-7c78-4154-8c9b-064dbde0ba6c"
   },
   "outputs": [],
   "source": [
    "from neuroai.utils.regression import ridge_regression\n",
    "\n",
    "ridge_result = ridge_regression(\n",
    "    X_train=all_features_flattened,\n",
    "    Y_train=all_fmri_voxels_region_mean,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(ridge_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ffac1-8103-453f-b44c-3e0e5e51382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## or train on all voxels\n",
    "# from neuroai.utils.regression import ridge_regression\n",
    "\n",
    "# ridge_result = ridge_regression(\n",
    "#     X_train=all_features_flattened,\n",
    "#     Y_train=data[\"voxels\"],\n",
    "#     device=\"cpu\"\n",
    "# )\n",
    "# # print(ridge_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "383e3f4f",
   "metadata": {
    "id": "383e3f4f"
   },
   "outputs": [],
   "source": [
    "from neuroai.utils.regression import RidgeModel\n",
    "\n",
    "model = RidgeModel(\n",
    "    backbone_model=backbone_model,\n",
    "    transforms=backbone_model.transforms,\n",
    "    hook_layer_name=\"model.layer3.1.conv1\",\n",
    "    ridge_result=ridge_result,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69883585",
   "metadata": {
    "id": "69883585"
   },
   "source": [
    "## Now let's evaluate on data from another subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e475939",
   "metadata": {
    "id": "0e475939"
   },
   "outputs": [],
   "source": [
    "valid_subject_ids = [\"s1\", \"s2\", \"s5\", \"s7\"]\n",
    "\n",
    "subject_id = \"s1\"\n",
    "\n",
    "dataset = NSDAllSubjectSingleRegion(\n",
    "    folder=\"./datasets/nsd\",\n",
    "    region=region,\n",
    "    transforms=backbone_model.transforms,\n",
    "    subset=\"test\", ## << make sure that this is set to \"test\" when evaluating\n",
    "    train_test_split=0.8\n",
    ")\n",
    "\n",
    "data = collect_features_and_voxels(\n",
    "    dataset=dataset,\n",
    "    backbone_model=backbone_model,\n",
    "    hook=hook,\n",
    "    device=device,\n",
    "    subject_id=subject_id\n",
    ")\n",
    "all_features_flattened = rearrange(\n",
    "    data[\"features\"],\n",
    "    \"batch channels height width -> batch (channels height width)\"\n",
    ")\n",
    "all_fmri_voxels_region_mean = reduce(\n",
    "    data[\"voxels\"],\n",
    "    pattern = \"batch voxels -> batch\",\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "print(all_features_flattened.shape, all_fmri_voxels_region_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70819a9",
   "metadata": {
    "id": "d70819a9"
   },
   "outputs": [],
   "source": [
    "## warning, this works only when you train the model on the mean of the ROIs, because different subjects have different numbers of voxels\n",
    "correlation = model.evaluate(\n",
    "    x_test=all_features_flattened,\n",
    "    y_test=all_fmri_voxels_region_mean,\n",
    ")\n",
    "print(f\"Correlation on another subject {subject_id}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848a39b",
   "metadata": {
    "id": "7848a39b"
   },
   "source": [
    "## Running the model on your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47136c8d",
   "metadata": {
    "id": "47136c8d"
   },
   "outputs": [],
   "source": [
    "# !wget -O cat.jpg \"https://plus.unsplash.com/premium_photo-1667030474693-6d0632f97029\"\n",
    "!wget -O castle.jpg \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Panorámica_Otoño_Alcázar_de_Segovia.jpg/1200px-Panorámica_Otoño_Alcázar_de_Segovia.jpg\"\n",
    "!wget -O monkey.jpg \"https://images.unsplash.com/photo-1581828060707-37894f1ed9b8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0fd2603",
   "metadata": {
    "id": "d0fd2603"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "images = [\n",
    "        Image.open(\"monkey.jpg\"),\n",
    "        Image.open(\"castle.jpg\"),\n",
    "        Image.open(\"castle.jpg\")\n",
    "    ]\n",
    "\n",
    "results = model.run(\n",
    "    images = images\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937f579-73f8-4e9e-a08e-72f8436c5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.utils.rdm import rdm_from_predictions\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "rdm = rdm_from_predictions(\n",
    "    fmri_predictions = torch.tensor(results), \n",
    "    mode = \"scipy\",\n",
    "    device = \"cpu\"\n",
    ")\n",
    "sns.heatmap(rdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iqyfZIAyJahE",
   "metadata": {
    "id": "iqyfZIAyJahE"
   },
   "source": [
    "## Bonus: Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4415b4b8",
   "metadata": {
    "collapsed": true,
    "id": "4415b4b8",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install torch-dreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sULsgUz7Jcym",
   "metadata": {
    "id": "sULsgUz7Jcym"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torch_dreams import Dreamer\n",
    "\n",
    "dreamy_boi = Dreamer(model, device = device, quiet=False)\n",
    "\n",
    "\n",
    "def maximize_voxel(layer_outputs):\n",
    "    loss = layer_outputs[0].mean()\n",
    "    return -loss\n",
    "\n",
    "image_param = dreamy_boi.render(\n",
    "    layers = [model],\n",
    "    width =224,\n",
    "    height =224,\n",
    "    custom_func = maximize_voxel,\n",
    "    scale_min=1.0,\n",
    "    scale_max=1.0,\n",
    "    iters=300,\n",
    "    translate_x=0.5,\n",
    "    translate_y=0.5,\n",
    "    rotate_degrees=5\n",
    ")\n",
    "\n",
    "plt.imshow(image_param)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "Nn8pVxGrLTXT",
   "metadata": {
    "id": "Nn8pVxGrLTXT"
   },
   "outputs": [],
   "source": [
    "image_param.save(\"visualization_eba.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abf9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
