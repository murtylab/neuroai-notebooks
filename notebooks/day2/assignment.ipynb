{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a26152-a5d6-43f8-ae9c-cefbf77774fc",
   "metadata": {},
   "source": [
    "## Assignment: finding the best ANN layer for different brain regions\n",
    "\n",
    "1. Pick ~5 layers in your model (try picking the mid/late layers since they yield smaller, features)\n",
    "2. Collect their features\n",
    "3. For each region of interest:\n",
    "    3.1 train a ridge regression model\n",
    "    3.2 evaluate it on a held-out subject\n",
    "    3.3 keep a log of the correlation score somewhere\n",
    "4. Find the layer with the highest correlation score for each region\n",
    "\n",
    "Ideally, make a bar plot with your results where X = layer name and Y = correlation score. One plot for each region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393aafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.datasets import NSDAllSubjectSingleRegion\n",
    "from neuroai.utils import ForwardHook\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from neuroai.utils.regression import RidgeModel, ridge_regression\n",
    "\n",
    "## here's a bunch of functions from the other notebook to make your life easier\n",
    "def collect_features_and_voxels(\n",
    "    dataset: NSDAllSubjectSingleRegion, \n",
    "    backbone_model: nn.Module, \n",
    "    hook: ForwardHook, \n",
    "    device: str, \n",
    "    subject_id: str\n",
    "):\n",
    "    \n",
    "    ## this is where we dump our data\n",
    "    all_features = []\n",
    "    all_fmri_voxels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            image: torch.Tensor = dataset[i][\"image\"]\n",
    "\n",
    "            ## (voxels) -> (1, voxels)\n",
    "            fmri_data: torch.Tensor = dataset[i][\"fmri_response\"][subject_id].unsqueeze(0)\n",
    "\n",
    "            ## (channels, height, width) -> (1, channels, height, width)\n",
    "            image = image.unsqueeze(0)\n",
    "            image = image.to(device)\n",
    "            y = backbone_model(image)\n",
    "\n",
    "            ## making sure that we're moving stuff back to the RAM with .cpu()\n",
    "            hook_output = hook.output.cpu()\n",
    "            all_features.append(hook_output)\n",
    "            all_fmri_voxels.append(fmri_data.cpu())\n",
    "\n",
    "    return {\n",
    "        \"features\": torch.cat(all_features, dim=0),\n",
    "        \"voxels\": torch.cat(all_fmri_voxels, dim=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroai.models import CLIPRN50\n",
    "\n",
    "backbone_model = CLIPRN50(pretrained=True, download_root=\"./pretrained_checkpoints\")\n",
    "backbone_model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "backbone_model = backbone_model.to(device)\n",
    "\n",
    "## first, look at the model and select a few layers\n",
    "print(backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"FFA\"\n",
    "\n",
    "layers_to_evaluate = [\n",
    "    ## select a few layers to evaluate over\n",
    "    \"model.layer4.0.conv1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698081a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NSDAllSubjectSingleRegion(\n",
    "    folder=\"./datasets/nsd\",\n",
    "    region=region,\n",
    "    transforms=backbone_model.transforms,\n",
    "    subset=\"train\",\n",
    "    train_test_split=0.8\n",
    ")\n",
    "\n",
    "features_from_different_layers = {}\n",
    "\n",
    "for layer_name in layers_to_evaluate:\n",
    "    print(f\"Processing layer {layer_name}\")\n",
    "\n",
    "    hook = ForwardHook(\n",
    "        model=backbone_model,\n",
    "        hook_layer_name=layer_name\n",
    "    )\n",
    "\n",
    "    collected_data = collect_features_and_voxels(\n",
    "        dataset=dataset,\n",
    "        backbone_model=backbone_model,\n",
    "        hook=hook,\n",
    "        device=device,\n",
    "        subject_id=\"s1\"\n",
    "    )\n",
    "\n",
    "    features = collected_data[\"features\"]\n",
    "    voxels = collected_data[\"voxels\"]\n",
    "\n",
    "    print(f\"Features shape: {features.shape}, Voxels shape: {voxels.shape}\")\n",
    "\n",
    "    features_from_different_layers[layer_name] = features\n",
    "    hook.close()  # clean up the hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146dc95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
